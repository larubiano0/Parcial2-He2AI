
# Parcial2 - He2AI

## Autores
[**Luis Alejandro Rubiano**]()

[**Nicolas Martinez Velez**]()

[**Carlos Andres Castillo**]()

[**Catalina Campos**]()

[**Manuela Pineda**]()

## üìö Descripci√≥n  
Este proyecto aplica **redes neuronales** para predecir la categor√≠a del PIB de los pa√≠ses, utilizando datos hist√≥ricos del World Bank Group. Se transforma un problema de regresi√≥n, basado en valores num√©ricos del PIB, en un problema de **clasificaci√≥n m√∫ltiple** dividiendo los pa√≠ses en tres grupos: **Low**, **Medium** y **High GDP**. La metodolog√≠a abarca desde el preprocesamiento y la transformaci√≥n de datos hasta la implementaci√≥n y evaluaci√≥n de diversos modelos de redes neuronales.

## üéØ Planteamiento del Problema
El objetivo de este proyecto es clasificar a los pa√≠ses, en funci√≥n de su PIB hist√≥rico (1960-2022), en tres categor√≠as: **Low GDP**, **Medium GDP** y **High GDP**. Este es un problema **supervisado** de **clasificaci√≥n m√∫ltiple**, en el que la variable objetivo **(GDP_category)** toma uno de tres valores. 

Se busca determinar cu√°l de los modelos de redes neuronales probados ‚ÄìRN Tradicional, RN Profunda‚Äì permite una mejor clasificaci√≥n, y se analiza tambi√©n el impacto de configuraciones sub√≥ptimas en el desempe√±o -RN con mala funci√≥n de p√©rdida-.

## ü§ñ Algoritmos Implementados
A continuaci√≥n, se describen los modelos de redes neuronales implementados en este proyecto:

1. **Red Neuronal Tradicional con Scikit-Learn**:  
   Es una implementaci√≥n de un Perceptr√≥n Multicapa (MLP) utilizando Scikit-Learn (MLPClassifier o MLPRegressor). La red cuenta con una capa de entrada, una o m√°s capas ocultas y una capa de salida, y ajusta sus pesos mediante algoritmos de optimizaci√≥n como el descenso de gradiente estoc√°stico (SGD) para minimizar la funci√≥n de p√©rdida. Este enfoque es f√°cil de usar y eficiente, aunque tiene limitaciones para construir redes profundas y ofrece menos opciones de personalizaci√≥n.

2. **Red Neuronal Profunda con TensorFlow**:  
   Utiliza TensorFlow para construir arquitecturas m√°s complejas y profundas. La red se define como una secuencia de capas (Layers) interconectadas, donde los datos fluyen a trav√©s de un grafo computacional. Durante el entrenamiento se aplica la retropropagaci√≥n para actualizar pesos utilizando optimizadores como Adam, permitiendo aprovechar hardware especializado (por ejemplo, GPUs) y obtener mayor flexibilidad en el dise√±o del modelo.

3. **Red Neuronal con Funci√≥n de P√©rdida Inadecuada**:  
   Este modelo se configura intencionalmente con una funci√≥n de p√©rdida inapropiada (por ejemplo, MSE en un problema de clasificaci√≥n) y un learning rate demasiado alto. Este enfoque permite observar c√≥mo una elecci√≥n inadecuada de la funci√≥n de p√©rdida y de los hiperpar√°metros afecta negativamente la convergencia y el rendimiento del modelo, sirviendo como estudio de caso para la importancia de configurar correctamente estos par√°metros.

   ## üìÇ Contenido del Repositorio

- **GDP_script.ipynb**: El script principal para ejecutar todos los modelos y evaluar su rendimiento.
- **README.md**: Este archivo que est√°s leyendo.
- 
   
# üìñ Relatoria 

Cronolog√≠a del desarrollo

**Preguntas iniciales y problemas identificados**

* Transformar el PIB en una variable categ√≥rica: El parcial requer√≠a convertir un valor continuo (PIB) en tres categor√≠as (por ejemplo, alto, medio y bajo). Este cambio de paradigma implicaba revisar la forma de evaluar la precisi√≥n y requer√≠a planificar el preprocesamiento (normalizaci√≥n, logaritmos, etc.) para asegurar resultados consistentes.}

* Selecci√≥n de m√©tricas para la nueva variable objetivo: El equipo debati√≥ sobre cu√°l m√©trica ser√≠a la m√°s apropiada tras el cambio a clasificaci√≥n: exactitud (accuracy), F1-score, matriz de confusi√≥n y curva ROC, entre otras. Finalmente, acordamos enfocarnos en la precisi√≥n (accuracy) y complementar con la matriz de confusi√≥n y el F1-score, para entender mejor el desempe√±o del modelo.

* Normalizaci√≥n y preprocesamiento de datos: Reconocimos que no era trivial decidir si normalizar las variables num√©ricas antes o despu√©s de la conversi√≥n a clasificaci√≥n. Tras debatir, concluimos que lo √≥ptimo era realizar una normalizaci√≥n (por ejemplo, escala estandar o logar√≠tmica) antes de entrenar los modelos, conservando la integridad del dataset.

* Manejo de valores faltantes y coherencia temporal: Encontramos muchos datos faltantes (missing values) en a√±os espec√≠ficos o en pa√≠ses con registros parciales. Adem√°s, no ten√≠a sentido utilizar informaci√≥n de a√±os posteriores para predecir a√±os anteriores, as√≠ que tuvimos que reordenar cronol√≥gicamente el dataset y descartar (o interpolar) datos excesivamente incompletos.

* Integraci√≥n de un nuevo dataset de expectativa de vida: Decidimos agregar la variable de expectativa de vida para cumplir con el bono propuesto y darle mayor riqueza al an√°lisis. esto exigi√≥ m√°s coherencia temporal y m√°s ajustes en la limpieza de datos.

**Soluciones y conclusiones a las que llegamos**

* Interpolaci√≥n y promedio para missing values: Luego de discutir eliminar datos incompletos, concluimos que era mejor conservar la mayor cantidad de informaci√≥n posible. Adoptamos la interpolaci√≥n lineal seguida de un promedio para rellenar vac√≠os. Esta estrategia nos permiti√≥ reducir la p√©rdida de datos sin afectar dr√°sticamente su distribuci√≥n.

* Limitaci√≥n temporal (a√±o 1990 en adelante): Para garantizar coherencia entre la informaci√≥n de PIB y la de expectativa de vida, decidimos tomar datos desde 1990, descartando a√±os con demasiados valores faltantes.

* OneHotEncoding para variables categ√≥ricas: Implementamos la codificaci√≥n de variables como regiones, grupos de ingreso, continentes y hemisferios. Esto evit√≥ supuestos de ordinalidad inexistentes y facilit√≥ que los modelos de clasificaci√≥n procesaran efectivamente estas variables.

* Transformaci√≥n logar√≠tmica en el PIB: Para estabilizar la escala y mejorar la separaci√≥n entre categor√≠as, aplicamos logaritmos al PIB antes de clasificar en bajo, medio y alto.

* Selecci√≥n de m√©tricas de evaluaci√≥n: Consideramos la precisi√≥n (accuracy) como la m√©trica principal de desempe√±o, complement√°ndola con reportes de clasificaci√≥n y curvas ROC.

Conclusiones clave: El manejo de los datos result√≥ fundamental para el desarrollo. Aprendimos que la estructura temporal es cr√≠tica y que la imputaci√≥n adecuada de datos faltantes, acompa√±ada de normalizaci√≥n y codificaci√≥n correctas, facilita modelos m√°s robustos.

**Proceso de Construcci√≥n y Optimizaci√≥n de los Modelos**

+ **Red neuronal tradicional (Scikit-Learn - Catalina)**: Inicialmente mostr√≥ una precisi√≥n baja (56%). se procedio a una b√∫squeda detallada de hiperpar√°metros usando GridSearchCV, destacando especialmente la importancia de las iteraciones y la cantidad de capas ocultas. Tras estos ajustes, alcanzamos finalmente una precisi√≥n del 86% en el conjunto de prueba.

+ **Red neuronal profunda (TensorFlow - Nicol√°s)**: Se desarroll√≥ un modelo profundo utilizando TensorFlow y realiz√≥ una optimizaci√≥n exhaustiva con Keras Tuner. Nicol√°s defini√≥ una funci√≥n para explorar m√∫ltiples configuraciones t√©cnicas (capas ocultas, funciones de activaci√≥n, dropout, regularizaci√≥n L2 y tasa de aprendizaje), realizando 10 iteraciones autom√°ticas. La mejor configuraci√≥n obtenida incluy√≥ cuatro capas ocultas, funci√≥n de activaci√≥n ReLU, dropout moderado (0.2), regularizaci√≥n L2 leve y un learning rate de 0.01. Este proceso riguroso llev√≥ a obtener finalmente una precisi√≥n del 74% en el conjunto de prueba.

+ **Modelo mal configurado**: Implementamos intencionalmente un modelo incorrecto utilizando MLPRegressor con una funci√≥n de p√©rdida dise√±ada para regresi√≥n (cuadr√°tica), en lugar de una funci√≥n apropiada para clasificaci√≥n (como cross-entropy). Adem√°s, utilizamos un learning rate excesivamente alto, lo cual caus√≥ problemas serios de convergencia. Esta configuraci√≥n err√≥nea result√≥ en una precisi√≥n extremadamente baja (33%), demostrando claramente la importancia t√©cnica cr√≠tica de elegir adecuadamente tanto la funci√≥n de p√©rdida como el learning rate seg√∫n la tarea espec√≠fica (clasificaci√≥n en este caso).

## **Divisi√≥n del trabajo** 
Todos los integrantes conocen y ejecutaron de manera independiente el codigo del proyecto. Sin embargo, cada uno lider√≥ una tarea distinta

  **Catalina:** Se centr√≥ en la red neuronal tradicional (Scikit-Learn). Inici√≥ con la primera arquitectura Perceptron y MLP y realiz√≥ los experimentos iniciales, encontrando una precisi√≥n inicial del 56%. hizo ajustes de hiperpar√°metros esenciales (n√∫mero de capas ocultas, neuronas y n√∫mero de iteraciones), logrando mejoras significativas en la precisi√≥n.

  **Nicol√°s:** Estuvo a cargo de la red neuronal profunda (TensorFlow), empleando Keras Tuner para la b√∫squeda de hiperpar√°metros. Prob√≥ distintas configuraciones (capas, dropout, funciones de activaci√≥n, regularizaci√≥n L2, tasas de aprendizaje, etc.) hasta encontrar una combinaci√≥n √≥ptima que llev√≥ a una precisi√≥n final de 74%. Asimismo dise√±√≥ el modelo mal configurado para ilustrar la importancia de usar la funci√≥n de p√©rdida adecuada. Adem√°s, lider√≥ el an√°lisis con SHAP Values, profundizando en la interpretabilidad de los modelos.

  **Luis:** Lider√≥ el preprocesamiento e integraci√≥n de datos, incluyendo la detecci√≥n y resoluci√≥n de valores faltantes, y la reorganizaci√≥n cronol√≥gica del dataset. Fue quien propuso y aplic√≥ OneHotEncoding a las variables categ√≥ricas, y la transformaci√≥n logar√≠tmica del PIB para estabilizar su distribuci√≥n. Tambien ayud√≥ en la b√∫squeda de hiperpar√°metros para el modelo tradicional mediante GridSearchCV. 

  **Manuela:** Asumi√≥ la documentaci√≥n del proyecto y la Relatoria de este, trato de mostrar con precisi√≥n las discusiones m√°s importantes y la l√≥gica tras cada decisi√≥n t√©cnica.

  **Carlos:** se encargo preguntas adicionales del parcial y los bonos, evaluando la coherencia de los resultados. Tambi√©n profundiz√≥ en el uso de herramientas de an√°lisis como la matriz de confusi√≥n, la curva ROC y los valores de SHAP, confirmando la solidez de las conclusiones. Realizo una relator√≠a completa del procesamiento de datos.
